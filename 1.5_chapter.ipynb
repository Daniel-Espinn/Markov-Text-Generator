{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb62a1c",
   "metadata": {},
   "source": [
    "# Markov Chain from scratch\n",
    "## Using pure python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21b59ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import operator\n",
    "import random\n",
    "import bisect\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe21f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Daniel Espin \n",
    "\n",
    "BEGIN = \"__BEGIN__\" # token especial para el incio de la secuencia\n",
    "END = \"__END__\" # token especial para el final de la secuencia\n",
    "\n",
    "def accumulate(iterable, func=operator.add):\n",
    "    # genera sumas acumulativas: [1, 2, 3] -> [1, 3, 6]\n",
    "    it = iter(iterable)\n",
    "    total = next(it)\n",
    "    yield total\n",
    "    for element in it:\n",
    "        total = func(total, element)\n",
    "        yield total\n",
    "\n",
    "def compile_next(next_dict):\n",
    "    words = list(next_dict.keys()) # Tokens posibles\n",
    "    cff = list(accumulate(next_dict.values())) # Sumas acumulativas de frecuencias\n",
    "    return [words, cff]\n",
    "\n",
    "class Chain:\n",
    "    def __init__(self, corpus, state_size, model=None):\n",
    "        \"\"\"\n",
    "        Corpus: Una lista de listas, donda cada lista externa es una ejecucion del proseso.\n",
    "        Y cada lista interna es una secuencia de los pasos de la ejecucion.\n",
    "        State size: Es un entero que indica el numero de items que usa el modelo para representar el estado (n-grama).\n",
    "        Ejemplo: [[\"A\",\"B\"], [\"C\",\"D\"]]\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.model = model or self.build(corpus, self.state_size)\n",
    "        self.compiled = (len(self.model) > 0) and (\n",
    "            type(self.model[tuple([BEGIN] * self.state_size)]) == list\n",
    "        )\n",
    "        if not self.compiled:\n",
    "            self.precompute_begin_state()\n",
    "\n",
    "    def compile(self, inplace=False):\n",
    "        if self.compiled:\n",
    "            if inplace:\n",
    "                return self\n",
    "            return Chain(None, self.state_size, model=copy.deepcopy(self.model))\n",
    "        mdict = {\n",
    "            state: compile_next(next_dict) for (state, next_dict) in self.model.items()\n",
    "        }\n",
    "        if not inplace:\n",
    "            return Chain(None, self.state_size, model=mdict)\n",
    "        self.model = mdict\n",
    "        self.compiled = True\n",
    "        return self\n",
    "\n",
    "    def build(self, corpus, state_size):\n",
    "        \"\"\"\n",
    "        Devuelve un diccionario de diccionarios donde la llave representa todos los estados posibles,\n",
    "        y apunta a los diccionarios internos que representan todas las posibilidades para el siguiente item en la cadena\n",
    "        \"\"\"\n",
    "\n",
    "        model = {}\n",
    "\n",
    "        for run in corpus:\n",
    "            items = ([BEGIN] * state_size) + run + [END]\n",
    "            for i in range(len(run) + 1):\n",
    "                state = tuple(items[i : i + state_size])\n",
    "                follow = items[i + state_size]\n",
    "                if state not in model:\n",
    "                    model[state] = {}\n",
    "                \n",
    "                if follow not in model[state]:\n",
    "                    model[state][follow] = 0\n",
    "\n",
    "                model[state][follow] += 1\n",
    "        return model\n",
    "    \n",
    "    def precompute_begin_state(self):\n",
    "        \"\"\"\n",
    "        Caches the summation calculation and available choices for BEGIN * state_size.\n",
    "        Significantly speeds up chain generation on large corpora. Thanks, @schollz!\n",
    "        \"\"\"\n",
    "        begin_state = tuple([BEGIN] * self.state_size)\n",
    "        choices, cumdist = compile_next(self.model[begin_state])\n",
    "        self.begin_cumdist = cumdist\n",
    "        self.begin_choices = choices\n",
    "\n",
    "    def move(self, state):\n",
    "        if self.compiled:\n",
    "            choices, cumdist = self.model[state]\n",
    "        elif state == tuple([BEGIN] * self.state_size):\n",
    "            choices = self.begin_choices\n",
    "            cumdist = self.begin_cumdist\n",
    "        else:\n",
    "            choices, weights = zip(*self.model[state].items())\n",
    "            cumdist = list(accumulate(weights))\n",
    "        r = random.random() * cumdist[-1]\n",
    "        selection = choices[bisect.bisect(cumdist, r)]\n",
    "        return selection\n",
    "    \n",
    "    def gen(self, init_state):\n",
    "        state = init_state or (BEGIN,) * self.state_size\n",
    "        while True:\n",
    "            next_word = self.move(state)\n",
    "            if next_word == END:\n",
    "                break\n",
    "            yield next_word\n",
    "            state = tuple(state[1:]) + (next_word,)\n",
    "\n",
    "    def walk(self, init_state=None):\n",
    "        return list(self.gen(init_state))\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(list(self.model.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_thing):\n",
    "        obj = json.loads(json_thing) if isinstance(json_thing, str) else json_thing\n",
    "\n",
    "        if isinstance(obj, list):\n",
    "            rehydrated = {tuple(item[0]): item[1] for item in obj}\n",
    "        elif isinstance(obj, dict):\n",
    "            rehydrated = obj\n",
    "        else:\n",
    "            raise ValueError(\"Object should be dict or list\")\n",
    "\n",
    "        state_size = len(list(rehydrated.keys())[0])\n",
    "\n",
    "        inst = cls(None, state_size, rehydrated)\n",
    "        return inst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a111f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', 'mundo']\n"
     ]
    }
   ],
   "source": [
    "corpus = [[\"Hola\", \"mundo\"], [\"Hola\", \"Python\"], [\"Hola\", \"IA\"]]\n",
    "cadena = Chain(corpus, state_size=1)\n",
    "cadena.compile()\n",
    "print(cadena.walk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb93285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d0a3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_letter_pat = re.compile(r\"^[A-Z]$\", re.UNICODE)\n",
    "initialism_pat = re.compile(r\"^[A-Za-z0-9]{1,2}(\\.[A-Za-z0-9]{1,2})+\\.$\", re.UNICODE)\n",
    "\n",
    "abbr_capped = \"|\".join([\n",
    "    \"ala|ariz|ark|calif|colo|conn|del|fla|ga|ill|ind\",\n",
    "    \"kan|ky|la|md|mass|mich|minn|miss|mo|mont\",\n",
    "    \"neb|nev|okla|ore|pa|tenn|vt|va|wash|wis|wyo\",\n",
    "    \"u.s\",\n",
    "    \"mr|ms|mrs|msr|dr|gov|pres|sen|sens|rep|reps\",\n",
    "    \"prof|gen|messrs|col|sr|jf|sgt|mgr|fr|rev\",\n",
    "    \"jr|snr|atty|supt\",\n",
    "    \"ave|av|blvd|st|rd|hwy\",  # Añadido 'av'\n",
    "    \"jan|feb|mar|apr|jun|jul|aug|sep|sept|oct|nov|dec\"\n",
    "]).split(\"|\")\n",
    "\n",
    "abbr_lowercase = \"etc|v|vs|viz|al|pct\".split(\"|\")\n",
    "\n",
    "# Funciones corregidas\n",
    "def is_abbreviation(dotted_word):\n",
    "    if not dotted_word.endswith('.'):\n",
    "        return False\n",
    "        \n",
    "    clipped = dotted_word[:-1]\n",
    "    if not clipped:\n",
    "        return False\n",
    "        \n",
    "    if re.match(uppercase_letter_pat, clipped[0]):\n",
    "        if len(clipped) == 1:\n",
    "            return True\n",
    "        elif clipped.lower() in abbr_capped:  # Comparación en minúsculas\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if clipped in abbr_lowercase:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def is_sentence_ender(word):\n",
    "    if not word:\n",
    "        return False\n",
    "        \n",
    "    if re.match(initialism_pat, word) is not None:\n",
    "        return False\n",
    "        \n",
    "    if word[-1] in [\"?\", \"!\"]:\n",
    "        return True\n",
    "        \n",
    "    if len(re.sub(r\"[^A-Z]\", \"\", word)) > 1:\n",
    "        return True\n",
    "        \n",
    "    if word[-1] == \".\":\n",
    "        # Detectar números decimales (ej: 99.99)\n",
    "        if any(char.isdigit() for char in word[:-1]):\n",
    "            return False\n",
    "            \n",
    "        # Detectar enumeraciones (ej: 1.)\n",
    "        if len(word) > 1 and word[-2].isdigit():\n",
    "            return False\n",
    "            \n",
    "        # Detectar abreviaturas\n",
    "        if is_abbreviation(word):\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    if not text.strip():\n",
    "        return []\n",
    "        \n",
    "    potential_end_pat = re.compile(\n",
    "        r\"\".join([\n",
    "            r\"([\\w\\.'’&\\]\\)]+[\\.\\?!])\",   # Palabra con puntuación\n",
    "            r\"([‘’“”'\\\"\\)\\]]*)\",            # Comillas/paréntesis de cierre\n",
    "            r\"(\\s+(?![a-z\\-–—]))\"           # Espacios seguidos de no minúsculas\n",
    "        ]),\n",
    "        re.UNICODE\n",
    "    )\n",
    "    \n",
    "    dot_iter = re.finditer(potential_end_pat, text)\n",
    "    end_indices = []\n",
    "    \n",
    "    for x in dot_iter:\n",
    "        if is_sentence_ender(x.group(1)):\n",
    "            end_index = x.start() + len(x.group(1)) + len(x.group(2))\n",
    "            end_indices.append(end_index)\n",
    "    \n",
    "    if not end_indices:\n",
    "        return [text.strip()]\n",
    "    \n",
    "    spans = []\n",
    "    start = 0\n",
    "    for end in end_indices:\n",
    "        spans.append((start, end))\n",
    "        start = end\n",
    "    spans.append((start, len(text)))\n",
    "    \n",
    "    sentences = [text[start:end].strip() for start, end in spans]\n",
    "    return [s for s in sentences if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0492d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Iniciando pruebas del splitter de oraciones ===\n",
      "✓ Test 1 pasado\n",
      "✗ Test 2 fallado\n",
      "  Input:    'El precio es de $99.99. Pero hay un descuento del 10%.'\n",
      "  Esperado: ['El precio es de $99.99.', 'Pero hay un descuento del 10%.']\n",
      "  Resultado: ['El precio es de $99.99. Pero hay un descuento del 10%.']\n",
      "------------------------------------------------------------\n",
      "✓ Test 3 pasado\n",
      "✗ Test 4 fallado\n",
      "  Input:    'Esto es una abreviatura: etc. Y esto es el final de la oración.'\n",
      "  Esperado: ['Esto es una abreviatura: etc.', 'Y esto es el final de la oración.']\n",
      "  Resultado: ['Esto es una abreviatura: etc. Y esto es el final de la oración.']\n",
      "------------------------------------------------------------\n",
      "✓ Test 5 pasado\n",
      "✓ Test 6 pasado\n",
      "✓ Test 7 pasado\n",
      "✓ Test 8 pasado\n",
      "\n",
      "Resultado final: 6/8 tests exitosos\n"
     ]
    }
   ],
   "source": [
    "def test_sentence_splitter():\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"input\": \"El Dr. García vive en la Av. Siempre Viva. ¿En serio? Sí, en EE.UU.\",\n",
    "            \"expected\": [\n",
    "                \"El Dr. García vive en la Av. Siempre Viva.\",\n",
    "                \"¿En serio?\",\n",
    "                \"Sí, en EE.UU.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"El precio es de $99.99. Pero hay un descuento del 10%.\",\n",
    "            \"expected\": [\n",
    "                \"El precio es de $99.99.\",\n",
    "                \"Pero hay un descuento del 10%.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Visitamos Washington D.C. y N.Y. en enero. Hizo mucho frío!\",\n",
    "            \"expected\": [\n",
    "                \"Visitamos Washington D.C. y N.Y. en enero.\",\n",
    "                \"Hizo mucho frío!\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Esto es una abreviatura: etc. Y esto es el final de la oración.\",\n",
    "            \"expected\": [\n",
    "                \"Esto es una abreviatura: etc.\",\n",
    "                \"Y esto es el final de la oración.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Ph.D. en A.I. es difícil. Pero vale la pena!\",\n",
    "            \"expected\": [\n",
    "                \"Ph.D. en A.I. es difícil.\",\n",
    "                \"Pero vale la pena!\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Corre rápido! Gritó '¡Detente!'. Pero no escuchó.\",\n",
    "            \"expected\": [\n",
    "                \"Corre rápido!\",\n",
    "                \"Gritó '¡Detente!'.\",\n",
    "                \"Pero no escuchó.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Esto no debería dividir: 1. Primer punto 2. Segundo punto\",\n",
    "            \"expected\": [\n",
    "                \"Esto no debería dividir: 1. Primer punto 2. Segundo punto\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Hola mundo. Sin espacios después... ¿Ves? Así funciona.\",\n",
    "            \"expected\": [\n",
    "                \"Hola mundo.\",\n",
    "                \"Sin espacios después...\",\n",
    "                \"¿Ves?\",\n",
    "                \"Así funciona.\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    passed = 0\n",
    "    for i, test in enumerate(test_cases):\n",
    "        result = split_into_sentences(test[\"input\"])\n",
    "        if result == test[\"expected\"]:\n",
    "            print(f\"✓ Test {i+1} pasado\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"✗ Test {i+1} fallado\")\n",
    "            print(f\"  Input:    '{test['input']}'\")\n",
    "            print(f\"  Esperado: {test['expected']}\")\n",
    "            print(f\"  Resultado: {result}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"\\nResultado final: {passed}/{len(test_cases)} tests exitosos\")\n",
    "    return passed == len(test_cases)\n",
    "\n",
    "# Ejecutar pruebas\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Iniciando pruebas del splitter de oraciones ===\")\n",
    "    test_sentence_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3db742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from unidecode import unidecode\n",
    "\n",
    "DEFAULT_MAX_OVERLAP_RATIO = 0.7\n",
    "DEFAULT_MAX_OVERLAP_TOTAL = 15\n",
    "DEFAULT_TRIES = 10\n",
    "\n",
    "\n",
    "class ParamError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Text:\n",
    "    reject_pat = re.compile(r\"(^')|('$)|\\s'|'\\s|[\\\"(\\(\\)\\[\\])]\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_text,\n",
    "        state_size=2,\n",
    "        chain=None,\n",
    "        parsed_sentences=None,\n",
    "        retain_original=True,\n",
    "        well_formed=True,\n",
    "        reject_reg=\"\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_text: A string.\n",
    "        state_size: An integer, indicating the number of words in the model's state.\n",
    "        chain: A trained markovify.Chain instance for this text, if pre-processed.\n",
    "        parsed_sentences: A list of lists, where each outer list is a \"run\"\n",
    "              of the process (e.g. a single sentence), and each inner list\n",
    "              contains the steps (e.g. words) in the run. If you want to simulate\n",
    "              an infinite process, you can come very close by passing just one, very\n",
    "              long run.\n",
    "        retain_original: Indicates whether to keep the original corpus.\n",
    "        well_formed: Indicates whether sentences should be well-formed, preventing\n",
    "              unmatched quotes, parenthesis by default, or a custom regular expression\n",
    "              can be provided.\n",
    "        reject_reg: If well_formed is True, this can be provided to override the\n",
    "              standard rejection pattern.\n",
    "        \"\"\"\n",
    "\n",
    "        self.well_formed = well_formed\n",
    "        if well_formed and reject_reg != \"\":\n",
    "            self.reject_pat = re.compile(reject_reg)\n",
    "\n",
    "        can_make_sentences = parsed_sentences is not None or input_text is not None\n",
    "        self.retain_original = retain_original and can_make_sentences\n",
    "        self.state_size = state_size\n",
    "\n",
    "        if self.retain_original:\n",
    "            self.parsed_sentences = parsed_sentences or list(\n",
    "                self.generate_corpus(input_text)\n",
    "            )\n",
    "\n",
    "            # Rejoined text lets us assess the novelty of generated sentences\n",
    "            self.rejoined_text = self.sentence_join(\n",
    "                map(self.word_join, self.parsed_sentences)\n",
    "            )\n",
    "            self.chain = chain or Chain(self.parsed_sentences, state_size)\n",
    "        else:\n",
    "            if not chain:\n",
    "                parsed = parsed_sentences or self.generate_corpus(input_text)\n",
    "            self.chain = chain or Chain(parsed, state_size)\n",
    "\n",
    "    def compile(self, inplace=False):\n",
    "        if inplace:\n",
    "            self.chain.compile(inplace=True)\n",
    "            return self\n",
    "        cchain = self.chain.compile(inplace=False)\n",
    "        psent = None\n",
    "        if hasattr(self, \"parsed_sentences\"):\n",
    "            psent = self.parsed_sentences\n",
    "        return Text(\n",
    "            None,\n",
    "            state_size=self.state_size,\n",
    "            chain=cchain,\n",
    "            parsed_sentences=psent,\n",
    "            retain_original=self.retain_original,\n",
    "            well_formed=self.well_formed,\n",
    "            reject_reg=self.reject_pat,\n",
    "        )\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Returns the underlying data as a Python dict.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"state_size\": self.state_size,\n",
    "            \"chain\": self.chain.to_json(),\n",
    "            \"parsed_sentences\": self.parsed_sentences if self.retain_original else None,\n",
    "        }\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"\n",
    "        Returns the underlying data as a JSON string.\n",
    "        \"\"\"\n",
    "        return json.dumps(self.to_dict())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, obj, **kwargs):\n",
    "        return cls(\n",
    "            None,\n",
    "            state_size=obj[\"state_size\"],\n",
    "            chain=Chain.from_json(obj[\"chain\"]),\n",
    "            parsed_sentences=obj.get(\"parsed_sentences\"),\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_str):\n",
    "        return cls.from_dict(json.loads(json_str))\n",
    "\n",
    "    def sentence_split(self, text):\n",
    "        \"\"\"\n",
    "        Splits full-text string into a list of sentences.\n",
    "        \"\"\"\n",
    "        return split_into_sentences(text)\n",
    "\n",
    "    def sentence_join(self, sentences):\n",
    "        \"\"\"\n",
    "        Re-joins a list of sentences into the full text.\n",
    "        \"\"\"\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    word_split_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "    def word_split(self, sentence):\n",
    "        \"\"\"\n",
    "        Splits a sentence into a list of words.\n",
    "        \"\"\"\n",
    "        return re.split(self.word_split_pattern, sentence)\n",
    "\n",
    "    def word_join(self, words):\n",
    "        \"\"\"\n",
    "        Re-joins a list of words into a sentence.\n",
    "        \"\"\"\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def test_sentence_input(self, sentence):\n",
    "        \"\"\"\n",
    "        A basic sentence filter. The default rejects sentences that contain\n",
    "        the type of punctuation that would look strange on its own\n",
    "        in a randomly-generated sentence.\n",
    "        \"\"\"\n",
    "        if len(sentence.strip()) == 0:\n",
    "            return False\n",
    "        # Decode unicode, mainly to normalize fancy quotation marks\n",
    "        decoded = unidecode(sentence)\n",
    "        # Sentence shouldn't contain problematic characters\n",
    "        if self.well_formed and self.reject_pat.search(decoded):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def generate_corpus(self, text):\n",
    "        \"\"\"\n",
    "        Given a text string, returns a list of lists; that is, a list of\n",
    "        \"sentences,\" each of which is a list of words. Before splitting into\n",
    "        words, the sentences are filtered through `self.test_sentence_input`\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            sentences = self.sentence_split(text)\n",
    "        else:\n",
    "            sentences = []\n",
    "            for line in text:\n",
    "                sentences += self.sentence_split(line)\n",
    "        passing = filter(self.test_sentence_input, sentences)\n",
    "        runs = map(self.word_split, passing)\n",
    "        return runs\n",
    "\n",
    "    def test_sentence_output(self, words, max_overlap_ratio, max_overlap_total):\n",
    "        \"\"\"\n",
    "        Given a generated list of words, accept or reject it. This one rejects\n",
    "        sentences that too closely match the original text, namely those that\n",
    "        contain any identical sequence of words of X length, where X is the\n",
    "        smaller number of (a) `max_overlap_ratio` (default: 0.7) of the total\n",
    "        number of words, and (b) `max_overlap_total` (default: 15).\n",
    "        \"\"\"\n",
    "        # Reject large chunks of similarity\n",
    "        overlap_ratio = round(max_overlap_ratio * len(words))\n",
    "        overlap_max = min(max_overlap_total, overlap_ratio)\n",
    "        overlap_over = overlap_max + 1\n",
    "        gram_count = max((len(words) - overlap_max), 1)\n",
    "        grams = [words[i : i + overlap_over] for i in range(gram_count)]\n",
    "        for g in grams:\n",
    "            gram_joined = self.word_join(g)\n",
    "            if gram_joined in self.rejoined_text:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def make_sentence(self, init_state=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Attempts `tries` (default: 10) times to generate a valid sentence,\n",
    "        based on the model and `test_sentence_output`. Passes `max_overlap_ratio`\n",
    "        and `max_overlap_total` to `test_sentence_output`.\n",
    "\n",
    "        If successful, returns the sentence as a string. If not, returns None.\n",
    "\n",
    "        If `init_state` (a tuple of `self.chain.state_size` words) is not specified,\n",
    "        this method chooses a sentence-start at random, in accordance with\n",
    "        the model.\n",
    "\n",
    "        If `test_output` is set as False then the `test_sentence_output` check\n",
    "        will be skipped.\n",
    "\n",
    "        If `max_words` or `min_words` are specified, the word count for the\n",
    "        sentence will be evaluated against the provided limit(s).\n",
    "        \"\"\"\n",
    "        tries = kwargs.get(\"tries\", DEFAULT_TRIES)\n",
    "        mor = kwargs.get(\"max_overlap_ratio\", DEFAULT_MAX_OVERLAP_RATIO)\n",
    "        mot = kwargs.get(\"max_overlap_total\", DEFAULT_MAX_OVERLAP_TOTAL)\n",
    "        test_output = kwargs.get(\"test_output\", True)\n",
    "        max_words = kwargs.get(\"max_words\", None)\n",
    "        min_words = kwargs.get(\"min_words\", None)\n",
    "\n",
    "        if init_state is None:\n",
    "            prefix = []\n",
    "        else:\n",
    "            prefix = list(init_state)\n",
    "            for word in prefix:\n",
    "                if word == BEGIN:\n",
    "                    prefix = prefix[1:]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        for _ in range(tries):\n",
    "            words = prefix + self.chain.walk(init_state)\n",
    "            if (max_words is not None and len(words) > max_words) or (\n",
    "                min_words is not None and len(words) < min_words\n",
    "            ):\n",
    "                continue  # pragma: no cover # see coveragepy/issues/198\n",
    "            if test_output and hasattr(self, \"rejoined_text\"):\n",
    "                if self.test_sentence_output(words, mor, mot):\n",
    "                    return self.word_join(words)\n",
    "            else:\n",
    "                return self.word_join(words)\n",
    "        return None\n",
    "\n",
    "    def make_short_sentence(self, max_chars, min_chars=0, **kwargs):\n",
    "        \"\"\"\n",
    "        Tries making a sentence of no more than `max_chars` characters and optionally\n",
    "        no less than `min_chars` characters, passing **kwargs to `self.make_sentence`.\n",
    "        \"\"\"\n",
    "        tries = kwargs.get(\"tries\", DEFAULT_TRIES)\n",
    "\n",
    "        for _ in range(tries):\n",
    "            sentence = self.make_sentence(**kwargs)\n",
    "            if sentence and min_chars <= len(sentence) <= max_chars:\n",
    "                return sentence\n",
    "\n",
    "    def make_sentence_with_start(self, beginning, strict=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Tries making a sentence that begins with `beginning` string,\n",
    "        which should be a string of one to `self.state` words known\n",
    "        to exist in the corpus.\n",
    "\n",
    "        If strict == True, then markovify will draw its initial inspiration\n",
    "        only from sentences that start with the specified word/phrase.\n",
    "\n",
    "        If strict == False, then markovify will draw its initial inspiration\n",
    "        from any sentence containing the specified word/phrase.\n",
    "\n",
    "        **kwargs are passed to `self.make_sentence`\n",
    "        \"\"\"\n",
    "        split = tuple(self.word_split(beginning))\n",
    "        word_count = len(split)\n",
    "\n",
    "        if word_count == self.state_size:\n",
    "            init_states = [split]\n",
    "\n",
    "        elif 0 < word_count < self.state_size:\n",
    "            if strict:\n",
    "                init_states = [(BEGIN,) * (self.state_size - word_count) + split]\n",
    "\n",
    "            else:\n",
    "                init_states = self.find_init_states_from_chain(split)\n",
    "\n",
    "                random.shuffle(init_states)\n",
    "        else:\n",
    "            err_msg = (\n",
    "                f\"`make_sentence_with_start` for this model requires a string \"\n",
    "                f\"containing 1 to {self.state_size} words. \"\n",
    "                f\"Yours has {word_count}: {str(split)}\"\n",
    "            )\n",
    "            raise ParamError(err_msg)\n",
    "\n",
    "        for init_state in init_states:\n",
    "            output = self.make_sentence(init_state, **kwargs)\n",
    "            if output is not None:\n",
    "                return output\n",
    "        err_msg = (\n",
    "            f\"`make_sentence_with_start` can't find sentence beginning with {beginning}\"\n",
    "        )\n",
    "        raise ParamError(err_msg)\n",
    "\n",
    "    @functools.lru_cache(maxsize=1)\n",
    "    def find_init_states_from_chain(self, split):\n",
    "        \"\"\"\n",
    "        Find all chains that begin with the split when `self.make_sentence_with_start`\n",
    "        is called with strict == False.\n",
    "\n",
    "        This is a very expensive operation, so lru_cache caches the results of\n",
    "        the latest query in case `self.make_sentence_with_start` is called\n",
    "        repeatedly with the same beginning string.\n",
    "        \"\"\"\n",
    "        word_count = len(split)\n",
    "        return [\n",
    "            key\n",
    "            for key in self.chain.model.keys()\n",
    "            # check for starting with begin as well ordered lists\n",
    "            if tuple(filter(lambda x: x != BEGIN, key))[:word_count] == split\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def from_chain(cls, chain_json, corpus=None, parsed_sentences=None):\n",
    "        \"\"\"\n",
    "        Init a Text class based on an existing chain JSON string or object\n",
    "        If corpus is None, overlap checking won't work.\n",
    "        \"\"\"\n",
    "        chain = Chain.from_json(chain_json)\n",
    "        return cls(\n",
    "            corpus or None,\n",
    "            parsed_sentences=parsed_sentences,\n",
    "            state_size=chain.state_size,\n",
    "            chain=chain,\n",
    "        )\n",
    "\n",
    "\n",
    "class NewlineText(Text):\n",
    "    \"\"\"\n",
    "    A (usable) example of subclassing markovify.Text. This one lets you markovify\n",
    "    text where the sentences are separated by newlines instead of \". \"\n",
    "    \"\"\"\n",
    "\n",
    "    def sentence_split(self, text):\n",
    "        return re.split(r\"\\s*\\n\\s*\", text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
